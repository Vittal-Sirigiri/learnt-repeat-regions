{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "import haversine\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "# i used this library pynlpl for n-grams creation and frequency calculations\n",
    "import pynlpl\n",
    "from pynlpl.statistics import FrequencyList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanse the data and remove all extraneous information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleansing steps:\n",
    "Remove these characters: Period | ( | ) | - | :\n",
    "Remove these characters and replace with a space\n",
    "Lower case the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding and Standardising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abbreviations expanded:\n",
    "rd: road\n",
    "ngr: nagar\n",
    "nr: near\n",
    "apt: apartment\n",
    "apts: apartment\n",
    "opp: opposite\n",
    "extn: extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "expansions_dict = {' rd,': ' road,', ' rd ': ' road ',\n",
    "                   ' apts,': ' apartment,', ' apts ': ' apartment ',\n",
    "                   ' apt,': ' apartment,', ' apt ': ' apartment ',\n",
    "                   ' appts,': ' apartment,',' appts ': ' apartment ',\n",
    "                   'apartments':'apartment',\n",
    "                   ' ngr,': ' nagar,', ' ngr ': ' nagar ',\n",
    "                   ',opp ': ',opposite ', ' opp ': ' opposite ',\n",
    "                   ',nr ': ',near ', ' nr ': ' near ',\n",
    "                   ' extn,': ' extension,', ' extn ': ' extension ',\n",
    "                   ' & ': ' and ', '&': ' and ',\n",
    "                  ' th ': 'th '}\n",
    "expansions_re = re.compile('(%s)' % '|'.join(expansions_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_expansions(s, expansions_dict=expansions_dict):\n",
    "    def replace(match):\n",
    "        return expansions_dict[match.group(0)]\n",
    "    return expansions_re.sub(replace, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "romanSearch = re.compile(r'\\b(?=[MDCLXVI]+\\b)M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})\\b')\n",
    "\n",
    "def roman_to_int(inputRoman):\n",
    "    inputRoman = inputRoman.upper(  )\n",
    "    nums = {'M':1000, 'D':500, 'C':100, 'L':50, 'X':10, 'V':5, 'I':1}\n",
    "    result = 0\n",
    "    for i in range(len(inputRoman)):\n",
    "        try:\n",
    "            value = nums[inputRoman[i]]\n",
    "            # If the next place holds a larger number, this value is negative\n",
    "            if i+1 < len(inputRoman) and nums[inputRoman[i+1]] > value:\n",
    "                result -= value\n",
    "            else: \n",
    "                result += value\n",
    "        except:\n",
    "            continue\n",
    "    return(result)\n",
    "\n",
    "\n",
    "def roman_int_regex(inputAddr):\n",
    "    def roman_to_int_repl(match):\n",
    "        exclude = set([\"LLC\"])   # add any other strings you don't want to replace\n",
    "        if match.group(0) in exclude:\n",
    "            return match.group(0)\n",
    "        return str(roman_to_int(match.group(0)))\n",
    "    return(romanSearch.sub(roman_to_int_repl, inputAddr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanAndExpand(inputDF):\n",
    "    tmpAddress = [re.sub(' +',' ',re.sub('[-#.(:)\",]',' ', inputDF.loc[ix, 'customerAddress'].lower())) for ix in inputDF.index]\n",
    "    tmpAddress1 = [expand_expansions(tmpAddress[ix]) for ix in range(len(tmpAddress))]\n",
    "    tmpAddress2 = [roman_int_regex(tmpAddress1[ix]) for ix in range(len(tmpAddress1))]\n",
    "    inputDF['cleanAddress'] = tmpAddress2\n",
    "    return(inputDF)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanRoundData(inputDF):\n",
    "    cleanAddress = cleanAndExpand(inputDF)\n",
    "    cleanAddress['del_latRounded4'] = np.round(cleanAddress['deliveredLat'],4)\n",
    "    cleanAddress['del_lngRounded4'] = np.round(cleanAddress['deliveredLng'],4)\n",
    "    return(cleanAddress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempt to parallelise the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from functools import reduce\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBiGrams(inputAddress):\n",
    "    return([phrase.strip().split(\" \")[i] + \" \" + phrase.strip().split(\" \")[i+1] for phrase in inputAddress.split(',') if len(phrase.strip().split(\" \"))>1 for i in range(len(phrase.strip().split(\" \"))-1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTriGrams(inputAddress):\n",
    "    return([phrase.strip().split(\" \")[i] + \" \" + phrase.strip().split(\" \")[i+1] + \" \" + phrase.strip().split(\" \")[i+2] for phrase in inputAddress.split(',') if len(phrase.strip().split(\" \"))>1 for i in range(len(phrase.strip().split(\" \"))-2)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for given n-grams, get their frequency\n",
    "def getFreqList(biGrams):    \n",
    "    freqlist =  pynlpl.statistics.FrequencyList()\n",
    "    freqlist.append(biGrams)\n",
    "    return(freqlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get n-grams within 1 Levenshtein Distance\n",
    "def getNearestNgramsDict(freqlist):\n",
    "    nGramKeys = list(freqlist.dict().keys())\n",
    "    finalngramDict = {}\n",
    "    for i in range(len(nGramKeys)):\n",
    "        finalngramDict[nGramKeys[i]] = {}\n",
    "        finalngramDict[nGramKeys[i]]['levDis'] = {}\n",
    "        for j in range(0,len(nGramKeys)):\n",
    "            finalngramDict[nGramKeys[i]]['levDis'][nGramKeys[j]] = pynlpl.statistics.levenshtein(nGramKeys[i],nGramKeys[j])\n",
    "    \n",
    "    nearestnGramsDict = {}\n",
    "    for i in range(len(nGramKeys)):\n",
    "        nGramCount = 0\n",
    "        for key in finalngramDict[nGramKeys[i]]['levDis'].keys():\n",
    "            if finalngramDict[nGramKeys[i]]['levDis'][key] <= 1: # change this value if needed\n",
    "                nGramCount += freqlist[key]\n",
    "        nearestnGramsDict[nGramKeys[i]] = nGramCount\n",
    "    return(nearestnGramsDict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is data file and indices of the rounded lat lngs for processing\n",
    "# creates bigrams and counts including bigrams within 1 LD\n",
    "def createNearestNGrams(kom_licious_cleanAddress,cutoffLocs):\n",
    "    stime = time.time()\n",
    "    biGramListofLists = []\n",
    "    pool_bgoutput = multiprocessing.Pool()\n",
    "    for ix in cutoffLocs.index:\n",
    "        selIndex = kom_licious_cleanAddress[(kom_licious_cleanAddress.del_latRounded4==cutoffLocs.loc[ix,'del_latRounded4']) & (kom_licious_cleanAddress.del_lngRounded4==cutoffLocs.loc[ix,'del_lngRounded4'])].index\n",
    "\n",
    "        bgoutput = pool_bgoutput.map(getBiGrams, kom_licious_cleanAddress.loc[selIndex,'cleanAddress'].values)\n",
    "        bgoutput_concat = reduce(operator.concat, bgoutput)\n",
    "        biGramListofLists.append(bgoutput_concat)\n",
    "    freqListofLists = pool_bgoutput.map(getFreqList, biGramListofLists)\n",
    "    nearestNgramsListofLists = pool_bgoutput.map(getNearestNgramsDict, freqListofLists)\n",
    "\n",
    "    etime = time.time()\n",
    "    print(etime-stime)\n",
    "    return(nearestNgramsListofLists, freqListofLists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a file of bigrams and their counts. for debug essentially\n",
    "def getBigramsFile(kom_licious_cleanAddress, cutoffLocs, nearestNgramsListofLists, freqListofLists, outputFile):\n",
    "    with open(outputFile,'w') as inFile:\n",
    "        fields = ['lat','lng','biGram','latLngOccurence','biGramCount','nearbyBiGramCount','confidence%','index']\n",
    "        writer = csv.DictWriter(inFile, fieldnames=fields)\n",
    "        writer.writeheader()\n",
    "        for i, ix in enumerate(cutoffLocs.index):\n",
    "            selIndex = kom_licious_cleanAddress[(kom_licious_cleanAddress.del_latRounded4==cutoffLocs.loc[ix,'del_latRounded4']) & (kom_licious_cleanAddress.del_lngRounded4==cutoffLocs.loc[ix,'del_lngRounded4'])].index\n",
    "\n",
    "            for key in nearestNgramsListofLists[i].keys():\n",
    "                tmpDict = {}\n",
    "                tmpDict['lat'] = cutoffLocs.loc[ix,'del_latRounded4']\n",
    "                tmpDict['lng'] = cutoffLocs.loc[ix,'del_lngRounded4']\n",
    "                tmpDict['biGram'] = key\n",
    "                tmpDict['latLngOccurence'] = len(selIndex)\n",
    "                tmpDict['biGramCount'] = freqListofLists[i][key]\n",
    "                tmpDict['nearbyBiGramCount'] = nearestNgramsListofLists[i][key]\n",
    "                tmpDict['index'] = ix\n",
    "                tmpDict['confidence%'] = 100*tmpDict['nearbyBiGramCount'] / tmpDict['latLngOccurence']\n",
    "                writer.writerow(tmpDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a bigram, it finds other bigrams within a distance threshold\n",
    "# and computes confidence of that bigram\n",
    "def getAggBiGram(biGramVal):\n",
    "    tmp = {}\n",
    "    thresh_nearby = 0\n",
    "    thresh_aggregate = 0\n",
    "    for srcInd in testbiGrams[testbiGrams['biGram']==biGramVal].index:\n",
    "        cntr = 0\n",
    "        llcntr = 0\n",
    "        latList = []\n",
    "        lngList = []\n",
    "        # go through all entries found for the given bigram\n",
    "        for destInd in testbiGrams[testbiGrams['biGram']==biGramVal].index:\n",
    "            drift = haversine.haversine((testbiGrams.loc[srcInd,'lat'],testbiGrams.loc[srcInd,'lng']),(testbiGrams.loc[destInd,'lat'],testbiGrams.loc[destInd,'lng']))\n",
    "            if drift <= 0.15: # threshold distance in km. collate all entries within threshold\n",
    "                latList.append(testbiGrams.loc[destInd,'lat'])\n",
    "                lngList.append(testbiGrams.loc[destInd,'lng'])\n",
    "                cntr +=testbiGrams.loc[destInd,'biGramCount']\n",
    "                llcntr +=testbiGrams.loc[destInd,'latLngOccurence']\n",
    "        # idea is to pick the best bigram-latLng combo in terms of frequency\n",
    "        # note to self and Sagar: This bit can be changed to output all bigram-latLng combo\n",
    "        # and make a choice at another step based on confidence and frequency count.\n",
    "        if (cntr >= thresh_aggregate) and (testbiGrams.loc[srcInd,'nearbyBiGramCount'] >= thresh_nearby):\n",
    "            tmp['del_lat'] = testbiGrams.loc[srcInd,'lat']\n",
    "            tmp['del_lng'] = testbiGrams.loc[srcInd,'lng']\n",
    "            tmp['latLngOccurence'] = llcntr\n",
    "            tmp['biGramOccurence'] = testbiGrams[testbiGrams['biGram']==biGramVal]['biGramCount'].sum()\n",
    "            tmp['biGramCount'] = testbiGrams.loc[srcInd,'biGramCount']\n",
    "            tmp['nearbyBiGramCount'] = testbiGrams.loc[srcInd,'nearbyBiGramCount']\n",
    "            tmp['aggregateBiGramCount'] = cntr\n",
    "            tmp['agg_lat'] = np.mean(latList)\n",
    "            tmp['agg_lng'] = np.mean(lngList)\n",
    "            tmp['biGram'] = biGramVal\n",
    "            tmp['confidence%'] = 100*cntr/tmp['biGramOccurence']\n",
    "            thresh_aggregate = cntr\n",
    "            thresh_nearby = testbiGrams.loc[srcInd,'nearbyBiGramCount']\n",
    "    return(tmp)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bigrams - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unidecode import unidecode\n",
    "cleanPattern = re.compile(r'[\\s,-.\\'\"/\\\\\\n\\r]')\n",
    "hashNumber = re.compile(r'#(?=\\d)|(?<=\\d)#')\n",
    "newLine = re.compile(r'[\\n\\r]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data has to be extracted. similar to what was done in the repeatRegionCreation\n",
    "# ensure that the column names are correct. customerAddress, deliveredLat, deliveredLng are the 3 columns we need\n",
    "# rename columns accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFile = pd.read_csv(sourceDataFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all duplicated addresses to remove inbuilt bias towards n-grams in those addresses\n",
    "testFile['tmpAddress'] = [newLine.sub(', ',testFile.loc[ix,'Customer Address']) for ix in testFile.index]\n",
    "testFile['cleanedAddress'] = [unidecode(hashNumber.sub('',cleanPattern.sub('',testFile.loc[ix,'tmpAddress'].lower()))) for ix in testFile.index ]\n",
    "testFile = testFile[~testFile['cleanedAddress'].duplicated()].reset_index().copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFile.rename(columns={\"Completed Lat\":\"deliveredLat\",\"Completed Lng\":\"deliveredLng\",\"Customer Address\":\"customerAddress\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFileCleaned = cleanRoundData(testFile)\n",
    "testFileGrouped = testFileCleaned.groupby(['del_latRounded4','del_lngRounded4'])['cleanAddress'].count().reset_index().copy()\n",
    "# min number of addresses per rounded lat lng for next step\n",
    "cutoffLocs = testFileGrouped[testFileGrouped.cleanAddress>=1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearestNGrams, freqList = createNearestNGrams(testFileCleaned, cutoffLocs)\n",
    "getBigramsFile(testFileCleaned, cutoffLocs, nearestNGrams,freqList, bigramsFileOuputLocation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "testbiGrams = pd.read_csv(bigramsFileOuputLocation)\n",
    "biGramTotalOccurence = testbiGrams.groupby('biGram')['biGramCount'].sum().reset_index().copy()\n",
    "# min frequency of the bigram for further processing\n",
    "shortListBiGrams = biGramTotalOccurence[biGramTotalOccurence.biGramCount>=10].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5675580501556396\n"
     ]
    }
   ],
   "source": [
    "pool_bgoutput = multiprocessing.Pool()\n",
    "\n",
    "stime = time.time()\n",
    "with open(bigramsAggregationFileOutputLocation,'w') as inFile:\n",
    "    fields = ['del_lat','del_lng','agg_lat','agg_lng',\n",
    "              'biGram','latLngOccurence','biGramOccurence',\n",
    "              'biGramCount','nearbyBiGramCount','aggregateBiGramCount','confidence%']\n",
    "    writer = csv.DictWriter(inFile, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    output = pool_bgoutput.map(getAggBiGram,list(shortListBiGrams['biGram'].values))\n",
    "    writer.writerows(output)\n",
    "etime = time.time()\n",
    "print(etime-stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregateBigrams(shortListBiGrams, outputFile):\n",
    "    pool_bgoutput = multiprocessing.Pool()\n",
    "\n",
    "    stime = time.time()\n",
    "    with open(outputFile,'w') as inFile:\n",
    "        fields = ['del_lat','del_lng','agg_lat','agg_lng',\n",
    "                  'biGram','latLngOccurence','biGramOccurence',\n",
    "                  'biGramCount','nearbyBiGramCount','aggregateBiGramCount','confidence%']\n",
    "        writer = csv.DictWriter(inFile, fieldnames=fields)\n",
    "        writer.writeheader()\n",
    "        output = pool_bgoutput.map(getAggBiGram,list(shortListBiGrams['biGram'].values))\n",
    "        writer.writerows(output)\n",
    "    etime = time.time()\n",
    "    print(etime-stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created a single function to combine all the above steps\n",
    "def learn(gg3monthsNoRepeats, bigramsFile, aggregateBiGramsFile, learntFile):\n",
    "    gg3monthsNoRepeats = cleanRoundData(gg3monthsNoRepeats)\n",
    "    gg3monthsNoRepeatsGrouped = gg3monthsNoRepeats.groupby(['del_latRounded4','del_lngRounded4'])['cleanAddress'].count().reset_index().copy()\n",
    "    gg3monthsNoRepeatscutoffLocs = gg3monthsNoRepeatsGrouped[gg3monthsNoRepeatsGrouped.cleanAddress>=1].copy()\n",
    "    nearestNGrams, freqList = createNearestNGrams(gg3monthsNoRepeats, gg3monthsNoRepeatscutoffLocs)\n",
    "    getBigramsFile(gg3monthsNoRepeats, gg3monthsNoRepeatscutoffLocs, nearestNGrams,freqList, bigramsFile)\n",
    "    testbiGrams = pd.read_csv(bigramsFile)\n",
    "    biGramTotalOccurence = testbiGrams.groupby('biGram')['biGramCount'].sum().reset_index().copy()\n",
    "    shortListBiGrams = biGramTotalOccurence[biGramTotalOccurence.biGramCount>=10].copy()\n",
    "    # start aggregating bi-grams\n",
    "    aggregateBigrams(shortListBiGrams, aggregateBiGramsFile)\n",
    "    # write aggregate bigrams into standard format\n",
    "    bigramfile = pd.read_csv(aggregateBiGramsFile)\n",
    "    with open(learntFile,'w') as inFile:\n",
    "        for ix in bigramfile[bigramfile.confidence>80][bigramfile.aggregateBiGramCount>10].index:\n",
    "            inFile.write(str(bigramfile.loc[ix,'biGram']) + ' | ' + str(bigramfile.loc[ix,'del_lat']) + ' | ' + str(bigramfile.loc[ix,'del_lng']) + ' | ' + str(bigramfile.loc[ix,'confidence']) + ' | ' + str(bigramfile.loc[ix,'aggregateBiGramCount']) + '\\n')\n",
    "        inFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228.108323097229\n",
      "254.50580501556396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vittalsirigiri/anaconda3/envs/gPandas/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# learntFile is the txt file in the format that backend consumes learnt regions\n",
    "learn(gg3months, bigramsFileOuputLocation, bigramsAggregationFileOutputLocation, learntFileLocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gPandas",
   "language": "python",
   "name": "gpandas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
