{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "import json\n",
    "from haversine import haversine\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import time\n",
    "import sys, csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist_mat(taskLats, taskLngs, vehLats, vehLngs):\n",
    "  \n",
    "    # index 0 of tuple has lat\n",
    "    # index 1 of tuple has lng\n",
    "    sLats = np.array([[taskLats[k]  for k in range(len(taskLats))] for l in range(len(vehLats))])\n",
    "    sLngs = np.array([[taskLngs[k]  for k in range(len(taskLats))] for l in range(len(vehLats))])\n",
    "\n",
    "    dLats = np.array([[vehLats[l]  for k in range(len(taskLats))] for l in range(len(vehLats))])\n",
    "    dLngs = np.array([[vehLngs[l]  for k in range(len(taskLats))] for l in range(len(vehLats))])\n",
    "\n",
    "    R = 6371.0088\n",
    "\n",
    "    s_lat = sLats*np.pi/180.0                      \n",
    "    s_lng = np.deg2rad(sLngs)     \n",
    "    e_lat = np.deg2rad(dLats)                       \n",
    "    e_lng = np.deg2rad(dLngs)  \n",
    "\n",
    "    d = np.sin((e_lat - s_lat)/2)**2 + np.cos(s_lat)*np.cos(e_lat) * np.sin((e_lng - s_lng)/2)**2\n",
    "\n",
    "    return 2 *1000 * R * np.arcsin(np.sqrt(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = 'gourmetgarden'\n",
    "start_date = '2020-08-01'\n",
    "end_date = '2020-10-30'\n",
    "team_id = 'bengaluru'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different address componets get concatenated while extracting the data itself because I found issues with \n",
    "# dtypes changing. Pincodes became float sometimes and a .0 got appended\n",
    "# I concatenated rawAddress, city and pincode. If a client has other components like locality, state etc, please add them\n",
    "# order of concat is given below:\n",
    "# 1. rawAddress 2. Locality 3. subLocality 4. City 5. State 6. Pincode\n",
    "\n",
    "def extractTeamData(client_id, start_date, end_date, team_id, path):\n",
    "    page_token = \"\"\n",
    "    tasks_export = []\n",
    "    auth = HTTPBasicAuth(\"mara/personnel/xxxxxxx\", \"xxxxx-xxxxx\")\n",
    "    writeUrl = f'https://api.locus-api.com/v1/write-access?clientId={client_id}&duration=10'\n",
    "    authHeaders = {'Authorization': 'Basic xxxxxxxxxxxxxxxxxxxx'}\n",
    "    writeResp = requests.get(writeUrl, headers = authHeaders )\n",
    "    print(writeResp.status_code)\n",
    "    time.sleep(15)\n",
    "    sheetColumns = ['Created On', 'Task Id', 'Team Name', 'Order Completion Time',\n",
    "           'Geocoded Lat', 'Geocoded Lng', 'Completed Lat', 'Completed Lng',\n",
    "           'Customer Address', 'Source', 'Pincode', 'City', 'countryCode',\n",
    "           'confidence','date','hour','concatAddress']\n",
    "    outFile = path + 'latLngData_' + str(team_id) + '_' + str(client_id) + '_' + str(start_date) + '_' + str(end_date) +'.csv'\n",
    "    with open(outFile, \"w\") as output:\n",
    "        writer = csv.DictWriter(output, fieldnames=sheetColumns)\n",
    "        writer.writeheader()\n",
    "        while page_token is not None:\n",
    "\n",
    "            if page_token == \"\":\n",
    "                get_tasks_url = f\"https://locus-api.com/v1/client/{client_id}/\" \\\n",
    "                    f\"task?limit=2000&lowerTimestamp={start_date}T00:00:00.000%2B0530&timestamp=\" \\\n",
    "                    f\"{end_date}T23:59:59.000%2B0530&taskStatus=COMPLETED&carrierClient=\" \\\n",
    "                    f\"{client_id}&carrierTeam={team_id}\"\n",
    "            else:\n",
    "                get_tasks_url = f\"https://locus-api.com/v1/client/{client_id}/\" \\\n",
    "                    f\"task?limit=2000&lowerTimestamp={start_date}T00:00:00.000%2B0530&timestamp=\" \\\n",
    "                    f\"{end_date}T23:59:59.000%2B0530&taskStatus=COMPLETED&\" \\\n",
    "                    f\"pageToken={page_token}&carrierClient={client_id}&carrierTeam={team_id}\"\n",
    "\n",
    "            task_response = requests.get(get_tasks_url, auth=auth).json()\n",
    "            tasks = task_response.get(\"tasks\")\n",
    "            page_token = task_response.get(\"pageToken\")\n",
    "            print(page_token)\n",
    "\n",
    "            if tasks[0].get(\"taskGraph\").get(\"visits\")[1].get(\"locationOptions\")[0].get(\"locationAddress\").get(\"formattedAddress\") == None:\n",
    "                writeResp = requests.get(writeUrl, headers = authHeaders )\n",
    "                print(writeResp.status_code)\n",
    "                task_response = requests.get(get_tasks_url, auth=auth).json()\n",
    "                tasks = task_response.get(\"tasks\")\n",
    "                page_token = task_response.get(\"pageToken\")\n",
    "            else:\n",
    "                print('Already have write access')\n",
    "\n",
    "\n",
    "\n",
    "            for task in tasks:\n",
    "                try:\n",
    "                    if task.get(\"status\").get(\"location\") is not None:\n",
    "                        completed_lat = task.get(\"status\").get(\"location\").get(\"lat\")\n",
    "                        completed_lng = task.get(\"status\").get(\"location\").get(\"lng\")\n",
    "                    else:\n",
    "                        completed_lat, completed_lng = None\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "                    row = {\n",
    "                        \"Created On\": task.get(\"creationTime\"),\n",
    "                        \"Task Id\": task.get(\"taskId\"),\n",
    "                        \"Team Name\": task.get(\"carrierTeams\")[0].get(\"teamId\"),\n",
    "                        \"Order Completion Time\": task.get(\"status\").get(\"triggerTime\"),\n",
    "                        \"Geocoded Lat\": task.get(\"taskGraph\").get(\"visits\")[1].get(\"locationOptions\")[0].get(\n",
    "                            \"geometry\").get(\"latLng\").get(\n",
    "                            \"lat\"),\n",
    "                        \"Geocoded Lng\": task.get(\"taskGraph\").get(\"visits\")[1].get(\"locationOptions\")[0].get(\n",
    "                            \"geometry\").get(\"latLng\").get(\n",
    "                            \"lng\"),\n",
    "                        \"Completed Lat\": completed_lat,\n",
    "                        \"Completed Lng\": completed_lng,\n",
    "                        \"Customer Address\": task.get(\"taskGraph\").get(\"visits\")[1].get(\"locationOptions\")[0].get(\n",
    "                            \"locationAddress\").get(\n",
    "                            \"formattedAddress\"),\n",
    "                        \"Source\": task.get(\"taskGraph\").get(\"visits\")[1].get(\"locationOptions\")[\n",
    "                            0].get(\"geocodingMetadata\").get(\"provider\"),\n",
    "                        \"Pincode\": task.get(\"taskGraph\").get(\"visits\")[1].get(\"locationOptions\")[0].get(\"locationAddress\").get(\"pincode\"),\n",
    "                        \"City\": task.get(\"taskGraph\").get(\"visits\")[1].get(\"locationOptions\")[0].get(\"locationAddress\").get(\"city\"),\n",
    "                        \"countryCode\": task.get(\"taskGraph\").get(\"visits\")[1].get(\"locationOptions\")[0].get(\"locationAddress\").get(\"countryCode\"),\n",
    "                        \"confidence\":task.get(\"taskGraph\").get(\"visits\")[1].get(\"locationOptions\")[0].get(\"geocodingMetadata\").get(\"confidence\"),\n",
    "                        \"date\":pd.to_datetime(task.get(\"status\").get(\"triggerTime\")[:-5]).date(),\n",
    "                        \"hour\":pd.to_datetime(task.get(\"status\").get(\"triggerTime\")[:-5]).hour,\n",
    "                        \"concatAddress\":task.get(\"taskGraph\").get(\"visits\")[1].get(\"locationOptions\")[0].get(\"locationAddress\").get(\"formattedAddress\") + ' , ' + task.get(\"taskGraph\").get(\"visits\")[1].get(\"locationOptions\")[0].get(\"locationAddress\").get(\"city\") + ' , ' + task.get(\"taskGraph\").get(\"visits\")[1].get(\"locationOptions\")[0].get(\"locationAddress\").get(\"pincode\")\n",
    "                    }\n",
    "                    writer.writerow(row)\n",
    "                except:\n",
    "                    continue\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "2020-10-20T11:30:00.000Z/AAA36971\n",
      "Already have write access\n",
      "2020-10-10T11:30:00.000Z/AAA34570\n",
      "Already have write access\n",
      "2020-09-29T11:30:00.000Z/AAA32283\n",
      "Already have write access\n",
      "2020-09-17T11:30:00.000Z/AAA30123\n",
      "Already have write access\n",
      "2020-09-05T11:30:00.000Z/AAA27999\n",
      "Already have write access\n",
      "2020-08-23T11:30:00.000Z/AAA25794\n",
      "Already have write access\n",
      "2020-08-11T11:30:00.000Z/AAA23651\n",
      "Already have write access\n",
      "None\n",
      "Already have write access\n"
     ]
    }
   ],
   "source": [
    "extractTeamData(client_id=client_id, start_date=start_date, end_date=end_date, team_id=team_id, path='./gourmetgarden/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(inputFile):\n",
    "    data = pd.read_csv(inputFile)\n",
    "    data = data[~data.duplicated(['Task Id','Order Completion Time'])] # just to remove any possible task duplicates\n",
    "    data.reset_index(inplace=True)\n",
    "    data.fillna('',inplace=True)\n",
    "    # use the below line if address is not concatenated while extracting\n",
    "    # data['concatAddress'] = [data.loc[ix,'Customer Address'] +', ' + str(data.loc[ix,'Pincode']) +', ' + data.loc[ix,'City'] for ix in data.index]\n",
    "    # follow the country wise cleaning logic used in the backend for consistency\n",
    "    cleanPattern = re.compile(r'[\\s,-.\\'\"/\\\\\\n\\r]')\n",
    "    hashNumber = re.compile(r'#(?=\\d)|(?<=\\d)#') # remove hash if it is next to a number\n",
    "    data['cleanedAddress'] = [unidecode(hashNumber.sub('',cleanPattern.sub('',data.loc[ix,'concatAddress'].lower()))) for ix in data.index ]\n",
    "    tmp = data.groupby('cleanedAddress')['Task Id'].count().reset_index()\n",
    "    # minimum frequency of duplicated addresses is set here. In this instance, it is 3.\n",
    "    duplicatedAddress = list(tmp[tmp['Task Id']>2]['cleanedAddress'].unique())\n",
    "    return(duplicatedAddress, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnRepeatedAddresses(montData, duplicatedAddress, outputFileName, buffer=25, minRepeatedInstances=3):\n",
    "#     buffer = 25 # in metres\n",
    "#     minRepeatedInstances = 3 # minimum number of times an address has to repeat to be considered for learning \n",
    "    with open(outputFileName,'w') as outputFile:\n",
    "    #     outputFile.write('concatAddress,cleanAddress,learntLat,learntLng,learntFrom\\n')\n",
    "        for address in duplicatedAddress:\n",
    "            compLats = list(montData[montData['cleanedAddress']==address]['Completed Lat'])\n",
    "            compLngs = list(montData[montData['cleanedAddress']==address]['Completed Lng'])\n",
    "            dismat = get_dist_mat(compLats, compLngs, compLats, compLngs)\n",
    "            sourceIndex, destIndex = np.where(dismat<buffer)\n",
    "            freqCount = np.unique(sourceIndex, return_counts=True)\n",
    "            if np.max(freqCount[1]) >=minRepeatedInstances:\n",
    "                selectedIndex = np.argmax(freqCount[1])\n",
    "                finalLatLng = [(compLats[ix],compLngs[ix]) for ix in [destIndex[item] for item in np.where(sourceIndex==selectedIndex)[0]]]\n",
    "                learntLat, learntLng = np.mean(finalLatLng,axis=0)\n",
    "                concatAddress = montData[montData['cleanedAddress']==address]['concatAddress'].unique()[0]\n",
    "                outputFile.write(concatAddress + ' | ' +str(learntLat)+' | '+str(learntLng)+' | 100.0 | '+str(np.max(freqCount[1])) +' | ' + address+'\\n')\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# example for preparing data\n",
    "duplicatedAddresses, updatedDF = prepareData(sourceDataFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for learning repeat regions\n",
    "# writing to a txt file which is used by backend\n",
    "learnRepeatedAddresses(updatedDF, duplicatedAddresses, outputTxtFile, buffer=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the txt file for analysis/easy reading\n",
    "output50 = pd.read_csv(outputTxtFile, sep= '|', names = ['rawAddress','lat','lng','conf','freq','concatAd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gPandas",
   "language": "python",
   "name": "gpandas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
